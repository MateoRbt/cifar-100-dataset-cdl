{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "016e1149",
   "metadata": {},
   "source": [
    "# Imports and Configuration\n",
    "This cell imports necessary libraries and sets up basic configurations like the device (CPU/GPU), the dataset to use (CIFAR-100 only), output directories, model details, and the similarity threshold for defining overlapping classes. It also creates the required output folders, including the new `test_set` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dee01ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Using Cosine Similarity threshold: > 0.9 for overlap\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset, ConcatDataset\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import timm\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "DATASET_NAMES = ['CIFAR100']\n",
    "\n",
    "OUTPUT_BASE_DIR = 'feature_extraction'\n",
    "FEATURE_DIR = os.path.join(OUTPUT_BASE_DIR, 'features')\n",
    "\n",
    "NEW_DATASET_DIR = 'new_dataset'\n",
    "TEST_SET_DIR = 'test_set'\n",
    "\n",
    "os.makedirs(FEATURE_DIR, exist_ok=True)\n",
    "os.makedirs(NEW_DATASET_DIR, exist_ok=True)\n",
    "os.makedirs(TEST_SET_DIR, exist_ok=True)\n",
    "\n",
    "DINO_MODEL_NAME = 'vit_small_patch16_224.dino'\n",
    "FEATURE_DIM = 384\n",
    "BATCH_SIZE_FEATURES = 256\n",
    "\n",
    "SIMILARITY_THRESHOLD = 0.9\n",
    "print(f\"Using Cosine Similarity threshold: > {SIMILARITY_THRESHOLD} for overlap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4855a6e1",
   "metadata": {},
   "source": [
    "Data Loading and Transforms\n",
    "This cell defines the image transformations needed for basic image loading and for DINO feature extraction. It then loads the CIFAR-100 dataset, keeping the training and test sets separate for later saving, but creating concatenated versions with DINO transforms specifically for the feature extraction step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6e0fdad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading Dataset: CIFAR100 ---\n",
      "  Loaded CIFAR100 Train: 50000 samples.\n",
      "  Loaded CIFAR100 Test: 10000 samples.\n",
      "  Total samples for feature extraction: 60000 samples, 100 classes.\n"
     ]
    }
   ],
   "source": [
    "transform_basic = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "transform_features = transforms.Compose([\n",
    "    transforms.Resize(256, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "original_datasets = {}\n",
    "feature_extract_datasets = {}\n",
    "original_class_names = {}\n",
    "dataset_target_attrs = {}\n",
    "\n",
    "name = 'CIFAR100'\n",
    "print(f\"\\n--- Loading Dataset: {name} ---\")\n",
    "num_classes = None\n",
    "try:\n",
    "    base_dataset_train = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_basic)\n",
    "    base_dataset_test = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_basic)\n",
    "    original_datasets[f\"{name}_train\"] = base_dataset_train\n",
    "    original_datasets[f\"{name}_test\"] = base_dataset_test\n",
    "\n",
    "    feature_dataset_train = datasets.CIFAR100(root='./data', train=True, download=False, transform=transform_features)\n",
    "    feature_dataset_test = datasets.CIFAR100(root='./data', train=False, download=False, transform=transform_features)\n",
    "    feature_extract_datasets[name] = ConcatDataset([feature_dataset_train, feature_dataset_test])\n",
    "\n",
    "    num_classes = 100\n",
    "    original_class_names[name] = base_dataset_train.classes\n",
    "    dataset_target_attrs[name] = 'targets'\n",
    "\n",
    "    print(f\"  Loaded {name} Train: {len(original_datasets[f'{name}_train'])} samples.\")\n",
    "    print(f\"  Loaded {name} Test: {len(original_datasets[f'{name}_test'])} samples.\")\n",
    "    print(f\"  Total samples for feature extraction: {len(feature_extract_datasets[name])} samples, {num_classes} classes.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"  Could not load {name}: {e}. Ensure it's downloaded or set download=True.\")\n",
    "\n",
    "len_train_dataset = len(original_datasets[f\"{name}_train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abd2d44",
   "metadata": {},
   "source": [
    "DINO Model Setup\n",
    "This cell loads the pre-trained DINO Vision Transformer model (`vit_small_patch16_224.dino`) using the `timm` library, sets it to evaluation mode, moves it to the appropriate device (GPU/CPU), and defines a wrapper class `DINOFeatureExtractor` to easily extract features (specifically the CLS token) from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "702497f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DINO model: vit_small_patch16_224.dino\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading DINO model: {DINO_MODEL_NAME}\")\n",
    "try:\n",
    "    dino_model = timm.create_model(DINO_MODEL_NAME, pretrained=True)\n",
    "    dino_model.eval()\n",
    "    dino_model.to(DEVICE)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading DINO model: {e}. Check model name and internet connection.\")\n",
    "\n",
    "class DINOFeatureExtractor(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "             if hasattr(self.model, 'forward_features'):\n",
    "                 feats = self.model.forward_features(x)\n",
    "                 cls_token = feats[:, 0]\n",
    "             else:\n",
    "                  raise AttributeError(\"Model does not have 'forward_features' method.\")\n",
    "        return cls_token\n",
    "\n",
    "extractor = DINOFeatureExtractor(dino_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2070af37",
   "metadata": {},
   "source": [
    "Feature Extraction\n",
    "This cell performs the DINO feature extraction on the concatenated CIFAR-100 dataset (train + test). It checks if the feature and label files already exist to avoid re-computation. If not, it iterates through the data using a DataLoader, extracts features using the DINO model, and saves the resulting feature tensors (`X_all`) and corresponding labels (`y_all`) to files. Finally, it cleans up GPU memory if applicable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d686482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Extracting Features for: CIFAR100 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c88d80a51674f9ba3315f169f4d09d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting CIFAR100:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved all features (torch.Size([60000, 384])) to feature_extraction\\features\\CIFAR100_X_all_dino.pt\n",
      "  Saved all labels (torch.Size([60000])) to feature_extraction\\features\\CIFAR100_y_all.pt\n"
     ]
    }
   ],
   "source": [
    "name = 'CIFAR100'\n",
    "if name not in feature_extract_datasets:\n",
    "    print(f\"\\nSkipping feature extraction for {name} (not loaded).\")\n",
    "else:\n",
    "    print(f\"\\n--- Extracting Features for: {name} ---\")\n",
    "\n",
    "    feature_path = os.path.join(FEATURE_DIR, f\"{name}_X_all_dino.pt\")\n",
    "    label_path = os.path.join(FEATURE_DIR, f\"{name}_y_all.pt\")\n",
    "\n",
    "    if os.path.exists(feature_path) and os.path.exists(label_path):\n",
    "        print(f\"  Features for {name} already exist. Skipping extraction.\")\n",
    "    else:\n",
    "        all_feats = []\n",
    "        all_labels = []\n",
    "\n",
    "        current_loader = DataLoader(\n",
    "            feature_extract_datasets[name],\n",
    "            batch_size=BATCH_SIZE_FEATURES,\n",
    "            shuffle=False,\n",
    "            num_workers=2\n",
    "        )\n",
    "\n",
    "        for images, labels in tqdm(current_loader, desc=f\"Extracting {name}\"):\n",
    "            images = images.to(DEVICE)\n",
    "            feats = extractor(images)\n",
    "            all_feats.append(feats.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "        X_all = torch.cat(all_feats, dim=0)\n",
    "        y_all = torch.cat(all_labels, dim=0)\n",
    "\n",
    "        torch.save(X_all, feature_path)\n",
    "        torch.save(y_all, label_path)\n",
    "        print(f\"  Saved all features ({X_all.shape}) to {feature_path}\")\n",
    "        print(f\"  Saved all labels ({y_all.shape}) to {label_path}\")\n",
    "\n",
    "del dino_model, extractor, feature_extract_datasets\n",
    "if DEVICE == torch.device('cuda'):\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbb958b",
   "metadata": {},
   "source": [
    "Overlap Identification\n",
    "This cell identifies which classes in CIFAR-100 are considered \"overlapping\" based on their DINO features. It loads the previously extracted features and labels, calculates the mean feature vector (centroid) for each class, computes the pairwise cosine similarity between all class centroids, and identifies pairs with similarity exceeding the defined `SIMILARITY_THRESHOLD`. The indices of these overlapping classes are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46c9ff5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Analyzing Feature Overlap for: CIFAR100 ---\n",
      "  Calculating centroids for 100 classes...\n",
      "  Calculating 100x100 similarity matrix...\n",
      "  Found 70 overlapping pairs.\n",
      "  Total unique overlapping classes: 54\n"
     ]
    }
   ],
   "source": [
    "class_centroids = {}\n",
    "overlapping_classes = {}\n",
    "\n",
    "name = 'CIFAR100'\n",
    "if name not in original_class_names:\n",
    "    print(f\"\\nSkipping overlap analysis for {name} (data not loaded).\")\n",
    "else:\n",
    "    print(f\"\\n--- Analyzing Feature Overlap for: {name} ---\")\n",
    "\n",
    "    try:\n",
    "        feature_path = os.path.join(FEATURE_DIR, f\"{name}_X_all_dino.pt\")\n",
    "        label_path = os.path.join(FEATURE_DIR, f\"{name}_y_all.pt\")\n",
    "\n",
    "        all_features = torch.load(feature_path)\n",
    "        all_labels = torch.load(label_path)\n",
    "        num_classes = len(original_class_names[name])\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  Feature/label files not found for {name}. Skipping analysis.\")\n",
    "\n",
    "    if 'all_features' in locals():\n",
    "        print(f\"  Calculating centroids for {num_classes} classes...\")\n",
    "        current_centroids = {}\n",
    "        for i in range(num_classes):\n",
    "            class_mask = (all_labels == i)\n",
    "            if class_mask.sum() == 0:\n",
    "                print(f\"  Warning: No samples found for class {i} in {name}. Skipping centroid.\")\n",
    "                continue\n",
    "\n",
    "            class_feats = all_features[class_mask]\n",
    "            current_centroids[i] = torch.mean(class_feats, dim=0)\n",
    "\n",
    "        class_centroids[name] = current_centroids\n",
    "\n",
    "        if len(current_centroids) < 2:\n",
    "            print(\"  Not enough class centroids to compare. Skipping similarity matrix.\")\n",
    "        else:\n",
    "            centroid_indices = sorted(current_centroids.keys())\n",
    "            centroid_tensor = torch.stack([current_centroids[idx] for idx in centroid_indices])\n",
    "\n",
    "            print(f\"  Calculating {len(centroid_indices)}x{len(centroid_indices)} similarity matrix...\")\n",
    "            sim_matrix = cosine_similarity(centroid_tensor.numpy())\n",
    "\n",
    "            current_overlapping_set = set()\n",
    "            overlapping_pairs_found = []\n",
    "\n",
    "            for i in range(len(centroid_indices)):\n",
    "                for j in range(i + 1, len(centroid_indices)):\n",
    "                    original_i = centroid_indices[i]\n",
    "                    original_j = centroid_indices[j]\n",
    "                    similarity = sim_matrix[i, j]\n",
    "\n",
    "                    if similarity > SIMILARITY_THRESHOLD:\n",
    "                        current_overlapping_set.add(original_i)\n",
    "                        current_overlapping_set.add(original_j)\n",
    "                        overlapping_pairs_found.append( (original_i, original_j, similarity) )\n",
    "\n",
    "            overlapping_classes[name] = current_overlapping_set\n",
    "\n",
    "            print(f\"  Found {len(overlapping_pairs_found)} overlapping pairs.\")\n",
    "            print(f\"  Total unique overlapping classes: {len(current_overlapping_set)}\")\n",
    "\n",
    "\n",
    "del all_features, all_labels, centroid_tensor, sim_matrix\n",
    "if DEVICE == torch.device('cuda'):\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a8e935",
   "metadata": {},
   "source": [
    "Saving Training Set Images\n",
    "This cell iterates through the original *training* set of CIFAR-100. For each image, it checks if its class label is in the set of overlapping classes identified previously. If it is, the image is converted to PIL format and saved as a PNG file in a subdirectory within `NEW_DATASET_DIR`, named according to the dataset and class name (e.g., `new_dataset/CIFAR100_apple`). Metadata for each saved image is collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0b9c9c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Creating New Training Dataset Subset in 'new_dataset' ---\n",
      "\n",
      "Processing overlapping classes for CIFAR100 (Training Set)...\n",
      "  Created/verified 54 subdirectories for CIFAR100 training set.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd10f96bf27343cf9d2e0cc84dc8bbca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving training images for CIFAR100:   0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved 27000 training samples for CIFAR100.\n",
      "\n",
      "Total training classes created/verified: 54\n",
      "Total training samples saved: 27000\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n--- Creating New Training Dataset Subset in '{NEW_DATASET_DIR}' ---\")\n",
    "\n",
    "metadata_train_list = []\n",
    "total_train_samples_saved = 0\n",
    "total_train_classes_saved = 0\n",
    "\n",
    "name = 'CIFAR100'\n",
    "if name not in overlapping_classes or not overlapping_classes[name]:\n",
    "    print(f\"\\nNo overlapping classes found/processed for {name}. Skipping training dataset creation.\")\n",
    "else:\n",
    "    print(f\"\\nProcessing overlapping classes for {name} (Training Set)...\")\n",
    "\n",
    "    base_dataset_train = original_datasets[f\"{name}_train\"]\n",
    "    class_names = original_class_names[name]\n",
    "    target_attr = dataset_target_attrs[name]\n",
    "    overlapping_set = overlapping_classes[name]\n",
    "\n",
    "    current_dataset_class_folders = set()\n",
    "    for class_idx in overlapping_set:\n",
    "        class_name = class_names[class_idx].replace(' ', '_').replace('/', '_')\n",
    "        new_folder_name = f\"{name}_{class_name}\"\n",
    "        new_class_dir = os.path.join(NEW_DATASET_DIR, new_folder_name)\n",
    "        os.makedirs(new_class_dir, exist_ok=True)\n",
    "        current_dataset_class_folders.add(new_folder_name)\n",
    "\n",
    "    total_train_classes_saved += len(current_dataset_class_folders)\n",
    "    print(f\"  Created/verified {len(current_dataset_class_folders)} subdirectories for {name} training set.\")\n",
    "\n",
    "    samples_saved_for_this_dataset = 0\n",
    "    for i in tqdm(range(len(base_dataset_train)), desc=f\"Saving training images for {name}\"):\n",
    "        try:\n",
    "            img_tensor, label_idx = base_dataset_train[i]\n",
    "\n",
    "            if label_idx in overlapping_set:\n",
    "                total_train_samples_saved += 1\n",
    "                samples_saved_for_this_dataset += 1\n",
    "\n",
    "                class_name = class_names[label_idx].replace(' ', '_').replace('/', '_')\n",
    "                new_folder_name = f\"{name}_{class_name}\"\n",
    "                new_class_dir = os.path.join(NEW_DATASET_DIR, new_folder_name)\n",
    "\n",
    "                img_pil = transforms.ToPILImage()(img_tensor)\n",
    "\n",
    "                img_filename = f\"{name}_train_orig-idx-{i}_label-{label_idx}.png\"\n",
    "                img_save_path = os.path.join(new_class_dir, img_filename)\n",
    "\n",
    "                img_pil.save(img_save_path)\n",
    "\n",
    "                metadata_train_list.append({\n",
    "                    'new_class_name': new_folder_name,\n",
    "                    'original_dataset': name,\n",
    "                    'original_class_name': class_names[label_idx],\n",
    "                    'original_label_idx': label_idx,\n",
    "                    'original_index_in_split': i,\n",
    "                    'split': 'train',\n",
    "                    'saved_path': img_save_path\n",
    "                })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  Error processing training sample index {i} for {name}: {e}\")\n",
    "\n",
    "    print(f\"  Saved {samples_saved_for_this_dataset} training samples for {name}.\")\n",
    "\n",
    "print(f\"\\nTotal training classes created/verified: {total_train_classes_saved}\")\n",
    "print(f\"Total training samples saved: {total_train_samples_saved}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad44035b",
   "metadata": {},
   "source": [
    "Saving Test Set Images\n",
    "This cell performs the same saving process as the previous one, but specifically for the *test* set of CIFAR-100. Images belonging to overlapping classes are saved into subdirectories within the dedicated `TEST_SET_DIR` (e.g., `new_dataset/test_set/CIFAR100_apple`). Metadata for the saved test images is collected separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4244ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Creating New Test Dataset Subset in 'test_set' ---\n",
      "\n",
      "Processing overlapping classes for CIFAR100 (Test Set)...\n",
      "  Created/verified 54 subdirectories for CIFAR100 test set.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6fa1897e5384860916382a9cd60d93e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving test images for CIFAR100:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved 5400 test samples for CIFAR100.\n",
      "\n",
      "Total test classes created/verified: 54\n",
      "Total test samples saved: 5400\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n--- Creating New Test Dataset Subset in '{TEST_SET_DIR}' ---\")\n",
    "\n",
    "metadata_test_list = []\n",
    "total_test_samples_saved = 0\n",
    "total_test_classes_saved = 0\n",
    "\n",
    "name = 'CIFAR100'\n",
    "if name not in overlapping_classes or not overlapping_classes[name]:\n",
    "    print(f\"\\nNo overlapping classes found/processed for {name}. Skipping test dataset creation.\")\n",
    "else:\n",
    "    print(f\"\\nProcessing overlapping classes for {name} (Test Set)...\")\n",
    "\n",
    "    base_dataset_test = original_datasets[f\"{name}_test\"]\n",
    "    class_names = original_class_names[name]\n",
    "    target_attr = dataset_target_attrs[name]\n",
    "    overlapping_set = overlapping_classes[name]\n",
    "\n",
    "    current_dataset_class_folders_test = set()\n",
    "    for class_idx in overlapping_set:\n",
    "        class_name = class_names[class_idx].replace(' ', '_').replace('/', '_')\n",
    "        new_folder_name_test = f\"{name}_{class_name}\"\n",
    "        new_class_dir_test = os.path.join(TEST_SET_DIR, new_folder_name_test)\n",
    "        os.makedirs(new_class_dir_test, exist_ok=True)\n",
    "        current_dataset_class_folders_test.add(new_folder_name_test)\n",
    "\n",
    "    total_test_classes_saved += len(current_dataset_class_folders_test)\n",
    "    print(f\"  Created/verified {len(current_dataset_class_folders_test)} subdirectories for {name} test set.\")\n",
    "\n",
    "    samples_saved_for_this_dataset_test = 0\n",
    "    for i in tqdm(range(len(base_dataset_test)), desc=f\"Saving test images for {name}\"):\n",
    "        try:\n",
    "            img_tensor, label_idx = base_dataset_test[i]\n",
    "\n",
    "            if label_idx in overlapping_set:\n",
    "                total_test_samples_saved += 1\n",
    "                samples_saved_for_this_dataset_test += 1\n",
    "\n",
    "                class_name = class_names[label_idx].replace(' ', '_').replace('/', '_')\n",
    "                new_folder_name_test = f\"{name}_{class_name}\"\n",
    "                new_class_dir_test = os.path.join(TEST_SET_DIR, new_folder_name_test)\n",
    "\n",
    "                img_pil = transforms.ToPILImage()(img_tensor)\n",
    "\n",
    "                img_filename_test = f\"{name}_test_orig-idx-{i}_label-{label_idx}.png\"\n",
    "                img_save_path_test = os.path.join(new_class_dir_test, img_filename_test)\n",
    "\n",
    "                img_pil.save(img_save_path_test)\n",
    "\n",
    "                metadata_test_list.append({\n",
    "                    'new_class_name': new_folder_name_test,\n",
    "                    'original_dataset': name,\n",
    "                    'original_class_name': class_names[label_idx],\n",
    "                    'original_label_idx': label_idx,\n",
    "                    'original_index_in_split': i,\n",
    "                    'split': 'test',\n",
    "                    'saved_path': img_save_path_test\n",
    "                })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  Error processing test sample index {i} for {name}: {e}\")\n",
    "\n",
    "    print(f\"  Saved {samples_saved_for_this_dataset_test} test samples for {name}.\")\n",
    "\n",
    "\n",
    "print(f\"\\nTotal test classes created/verified: {total_test_classes_saved}\")\n",
    "print(f\"Total test samples saved: {total_test_samples_saved}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431f78ec",
   "metadata": {},
   "source": [
    "Save Training Metadata\n",
    "This cell takes the collected metadata for the saved *training* images and saves it into a CSV file named `metadata_train.csv` inside the `NEW_DATASET_DIR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10e88b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully saved training metadata CSV to: new_dataset\\metadata_train.csv\n"
     ]
    }
   ],
   "source": [
    "metadata_train_df = pd.DataFrame(metadata_train_list)\n",
    "csv_train_path = os.path.join(NEW_DATASET_DIR, 'metadata_train.csv')\n",
    "\n",
    "try:\n",
    "    metadata_train_df.to_csv(csv_train_path, index=False)\n",
    "    print(f\"\\nSuccessfully saved training metadata CSV to: {csv_train_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError saving training metadata CSV: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dfe79b",
   "metadata": {},
   "source": [
    "Save Test Metadata\n",
    "This cell takes the collected metadata for the saved *test* images and saves it into a CSV file named `metadata_test.csv` inside the `TEST_SET_DIR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b38c746f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully saved test metadata CSV to: test_set\\metadata_test.csv\n"
     ]
    }
   ],
   "source": [
    "metadata_test_df = pd.DataFrame(metadata_test_list)\n",
    "csv_test_path = os.path.join(TEST_SET_DIR, 'metadata_test.csv')\n",
    "\n",
    "try:\n",
    "    metadata_test_df.to_csv(csv_test_path, index=False)\n",
    "    print(f\"\\nSuccessfully saved test metadata CSV to: {csv_test_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError saving test metadata CSV: {e}\")\n",
    "\n",
    "del original_datasets # Clean up original data loaded into memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad8868",
   "metadata": {},
   "source": [
    "Final Report\n",
    "This cell prints a summary of the dataset creation process, reporting the locations of the new training and test subset directories, the total number of overlapping classes found, and the total number of training and test samples saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "568be4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Dataset Summary ---\n",
      "New training subset location: c:\\Users\\mateo\\Desktop\\cifar-100-dataset-cdl\\new_dataset\n",
      "New test subset location: c:\\Users\\mateo\\Desktop\\cifar-100-dataset-cdl\\test_set\n",
      "Total number of overlapping classes (folders created in each subset): 54\n",
      "Total number of training samples saved: 27000\n",
      "Total number of test samples saved: 5400\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Final Dataset Summary ---\")\n",
    "print(f\"New training subset location: {os.path.abspath(NEW_DATASET_DIR)}\")\n",
    "print(f\"New test subset location: {os.path.abspath(TEST_SET_DIR)}\")\n",
    "num_overlap_classes = len(overlapping_classes.get('CIFAR100', set()))\n",
    "print(f\"Total number of overlapping classes (folders created in each subset): {num_overlap_classes}\")\n",
    "print(f\"Total number of training samples saved: {total_train_samples_saved}\")\n",
    "print(f\"Total number of test samples saved: {total_test_samples_saved}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
