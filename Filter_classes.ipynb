{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27601be7",
   "metadata": {},
   "source": [
    "This cell imports the required libraries (torch, timm, torchvision, os, tqdm) and sets up key configurations. It defines the device (GPU/CPU), the paths to your new train and test sets, a new directory to save the extracted features, and the DINO model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62275865",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import timm\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "NEW_DATASET_DIR = 'new_dataset'\n",
    "TEST_SET_DIR = 'test_set'\n",
    "\n",
    "NEW_FEATURE_DIR = 'new_dataset_features'\n",
    "os.makedirs(NEW_FEATURE_DIR, exist_ok=True)\n",
    "print(f\"New features will be saved in: {NEW_FEATURE_DIR}\")\n",
    "\n",
    "DINO_MODEL_NAME = 'vit_small_patch16_224.dino'\n",
    "BATCH_SIZE_FEATURES = 256 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a64de9",
   "metadata": {},
   "source": [
    "This cell defines the exact same image transformations required for the DINO model. It's crucial to use the same resizing, cropping, and normalization as in your first notebook to ensure the features are calculated correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c690ad2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_features = transforms.Compose([\n",
    "    transforms.Resize(256, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "print(\"DINO feature extraction transforms defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8449925f",
   "metadata": {},
   "source": [
    "Here, we use torchvision.datasets.ImageFolder. This utility automatically loads images from sub-folders, treating each sub-folder name as a class label. It's perfect for the directory structure you created. It will load all images from new_dataset (e.g., CIFAR100_apple, CIFAR100_orange) as the training set and all images from new_dataset/test_set as the test set, applying the DINO transforms on the fly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1bdf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    train_dataset = datasets.ImageFolder(\n",
    "        root=NEW_DATASET_DIR,\n",
    "        transform=transform_features\n",
    "    )\n",
    "    \n",
    "    test_dataset = datasets.ImageFolder(\n",
    "        root=TEST_SET_DIR,\n",
    "        transform=  transform_features\n",
    "    )\n",
    "    \n",
    "    val_size = 0.3\n",
    "    train_size = 1 - val_size\n",
    "    train_indices, val_indices = train_test_split(\n",
    "        list(range(len(train_dataset))),\n",
    "        test_size=val_size,\n",
    "        random_state=42,\n",
    "        stratify=[train_dataset.samples[i][1] for i in range(len(train_dataset))]\n",
    "    )\n",
    "    val_dataset = torch.utils.data.Subset(train_dataset, val_indices)\n",
    "    train_dataset = torch.utils.data.Subset(train_dataset, train_indices)\n",
    "    \n",
    "    print(f\"Successfully loaded new training set from: {NEW_DATASET_DIR}\")\n",
    "    print(f\"  - Train samples: {len(train_dataset)}\")\n",
    "    print(f\"  - Train classes: {len(train_dataset.classes)}\")\n",
    "    \n",
    "    print(f\"\\nSuccessfully created validation set from training data.\")\n",
    "    print(f\"  - Validation samples: {len(val_dataset)}\")    \n",
    "    print(f\"  - Validation classes: {len(val_dataset.dataset.classes)}\")\n",
    "    \n",
    "    print(f\"\\nSuccessfully loaded new test set from: {TEST_SET_DIR}\")\n",
    "    print(f\"  - Test samples: {len(test_dataset)}\")\n",
    "    print(f\"  - Test classes: {len(test_dataset.classes)}\")\n",
    "\n",
    "    # The class names and their corresponding indices are stored here:\n",
    "    # print(\"\\nClass to index mapping:\")\n",
    "    # print(train_dataset.class_to_idx)\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: Could not find dataset directories.\")\n",
    "    print(f\"Please ensure '{NEW_DATASET_DIR}' and '{TEST_SET_DIR}' exist.\")\n",
    "    print(e)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading datasets: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c0b445",
   "metadata": {},
   "source": [
    "This cell wraps the train_dataset and test_dataset in DataLoader objects. This allows us to process the images in batches, which is much more efficient for the GPU. We set shuffle=False because the order doesn't matter for feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6beee5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'train_dataset' in locals() and 'test_dataset' in locals():\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE_FEATURES,\n",
    "        shuffle=False,\n",
    "        \n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=BATCH_SIZE_FEATURES,\n",
    "        shuffle=False,\n",
    "\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=BATCH_SIZE_FEATURES,\n",
    "        shuffle=False,\n",
    "       \n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Created DataLoaders for the new train ({len(train_loader)} batches), ({len(val_loader)}) and test ({len(test_loader)} batches) sets.\")\n",
    "else:\n",
    "    print(\"Datasets not loaded. Skipping DataLoader creation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35258160",
   "metadata": {},
   "source": [
    "This cell loads the pre-trained DINO Vision Transformer (vit_small_patch16_224.dino) using timm. We also redefine the DINOFeatureExtractor wrapper class from your original notebook to easily extract the CLS token (the feature vector) from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90605db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading DINO model: {DINO_MODEL_NAME}\")\n",
    "try:\n",
    "    dino_model = timm.create_model(DINO_MODEL_NAME, pretrained=True)\n",
    "    dino_model.eval()\n",
    "    dino_model.to(DEVICE)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading DINO model: {e}. Check model name and internet connection.\")\n",
    "\n",
    "class DINOFeatureExtractor(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "             if hasattr(self.model, 'forward_features'):\n",
    "                 feats = self.model.forward_features(x)\n",
    "                 cls_token = feats[:, 0] \n",
    "             else:\n",
    "                  raise AttributeError(\"Model does not have 'forward_features' method.\")\n",
    "        return cls_token\n",
    "\n",
    "extractor = DINOFeatureExtractor(dino_model)\n",
    "print(\"DINO model and feature extractor are ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e5a968",
   "metadata": {},
   "source": [
    "This cell defines a helper function extract_features. This function takes a DataLoader and the extractor model, iterates through all batches, collects the features and labels, and returns them as concatenated tensors. This avoids duplicating code for the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99e8c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(loader, extractor_model, device, desc=\"Extracting features\"):\n",
    "\n",
    "    all_feats = []\n",
    "    all_labels = []\n",
    "    \n",
    "    extractor_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc=desc):\n",
    "            images = images.to(device)\n",
    "            \n",
    "            feats = extractor_model(images)\n",
    "            \n",
    "            all_feats.append(feats.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "            \n",
    "    X_all = torch.cat(all_feats, dim=0)\n",
    "    y_all = torch.cat(all_labels, dim=0)\n",
    "    \n",
    "    return X_all, y_all\n",
    "\n",
    "print(\"Feature extraction helper function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00078e1",
   "metadata": {},
   "source": [
    "This is the final step. We call the extract_features function for both the train_loader and test_loader. The resulting feature tensors (X_train_new, y_train_new, etc.) are then saved to the new_dataset_features directory as .pt files. This gives you the DINO features for your new, overlapping-only dataset, ready for further analysis or model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015d3e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'train_loader' in locals() and 'test_loader' in locals():\n",
    "    print(\"--- Starting Feature Extraction for New Datasets ---\")\n",
    "    \n",
    "    X_train_new, y_train_new = extract_features(\n",
    "        train_loader, \n",
    "        extractor, \n",
    "        DEVICE, \n",
    "        desc=\"Extracting Train Features\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nFinished train extraction. Feature shape: {X_train_new.shape}, Label shape: {y_train_new.shape}\")\n",
    "    \n",
    "    X_test_new, y_test_new = extract_features(\n",
    "        test_loader, \n",
    "        extractor, \n",
    "        DEVICE, \n",
    "        desc=\"Extracting Test Features\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nFinished test extraction. Feature shape: {X_test_new.shape}, Label shape: {y_test_new.shape}\")\n",
    "    \n",
    "    X_val_new, y_val_new = extract_features(\n",
    "        val_loader,\n",
    "        extractor,\n",
    "        DEVICE,\n",
    "        desc=\"Extracting Validation Features\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nFinished validation extraction. Feature shape: {X_val_new.shape}, Label shape: {y_val_new.shape}\")\n",
    "    \n",
    "    train_feat_path = os.path.join(NEW_FEATURE_DIR, \"train_X_dino.pt\")\n",
    "    train_label_path = os.path.join(NEW_FEATURE_DIR, \"train_y.pt\")\n",
    "    test_feat_path = os.path.join(NEW_FEATURE_DIR, \"test_X_dino.pt\")\n",
    "    test_label_path = os.path.join(NEW_FEATURE_DIR, \"test_y.pt\")\n",
    "    val_feat_path = os.path.join(NEW_FEATURE_DIR, \"val_X_dino.pt\")\n",
    "    val_label_path = os.path.join(NEW_FEATURE_DIR, \"val_y.pt\")\n",
    "\n",
    "    try:\n",
    "        torch.save(X_train_new, train_feat_path)\n",
    "        torch.save(y_train_new, train_label_path)\n",
    "        print(f\"\\nSaved new training features to: {train_feat_path}\")\n",
    "        print(f\"Saved new training labels to: {train_label_path}\")\n",
    "        \n",
    "        torch.save(X_val_new, val_feat_path)\n",
    "        torch.save(y_val_new, val_label_path)\n",
    "        print(f\"Saved new validation features to: {val_feat_path}\")\n",
    "        print(f\"Saved new validation labels to: {val_label_path}\")   \n",
    "        \n",
    "        torch.save(X_test_new, test_feat_path)\n",
    "        torch.save(y_test_new, test_label_path)\n",
    "        print(f\"Saved new test features to: {test_feat_path}\")\n",
    "        print(f\"Saved new test labels to: {test_label_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError saving feature/label files: {e}\")\n",
    "\n",
    "    # Clean up memory\n",
    "    del dino_model, extractor, X_train_new, y_train_new, X_test_new, y_test_new\n",
    "    if DEVICE == torch.device('cuda'):\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "else:\n",
    "    print(\"Dataloaders not found. Aborting feature extraction.\")\n",
    "\n",
    "print(\"\\n--- Process Complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1caf0b11",
   "metadata": {},
   "source": [
    "Mlp run for further filtering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da224bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.load(train_feat_path)\n",
    "y_train = torch.load(train_label_path)\n",
    "X_val = torch.load(val_feat_path)\n",
    "y_val = torch.load(val_label_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e02c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from collections import Counter\n",
    "\n",
    "input_dim = 384\n",
    "y_train = torch.load(train_label_path)\n",
    "\n",
    "input_dim = 384\n",
    "num_classes = int(y_train.max().item() + 1)\n",
    "print(f\"Automatically detected {num_classes} classes.\") \n",
    "\n",
    "batch_size = 256\n",
    "num_epochs = 200\n",
    "lr = 1e-4\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_dim=384, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, delta=0.0):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_loss = float('inf')\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if val_loss < self.best_loss - self.delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "val_data = TensorDataset(X_val, y_val)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "class_counts = Counter(y_train.numpy())\n",
    "class_weights = [0] * num_classes\n",
    "for cls in range(num_classes):\n",
    "    class_weights[cls] = 1.0 / class_counts.get(cls, 1)  \n",
    "\n",
    "weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "model = SimpleMLP(input_dim=input_dim, num_classes=num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=weights) \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=5) \n",
    "early_stopper = EarlyStopping(patience=20, delta=1e-5)\n",
    "\n",
    "\n",
    "\n",
    "train_size = len(train_data)\n",
    "val_size = len(val_data)\n",
    "\n",
    "best_model_state = None\n",
    "best_val_loss = float('inf')\n",
    "best_train_loss = float('inf')\n",
    "learning_rates = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        preds = model(x_batch)\n",
    "        loss = criterion(preds, y_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * x_batch.size(0)\n",
    "    learning_rates.append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in val_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            preds = model(x_batch)\n",
    "            loss = criterion(preds, y_batch)\n",
    "\n",
    "            val_loss += loss.item() * x_batch.size(0)\n",
    "            \n",
    "\n",
    "    avg_train_loss = train_loss / train_size\n",
    "    avg_val_loss = val_loss / val_size\n",
    "    scheduler.step(avg_val_loss)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        best_model_state = model.state_dict()\n",
    "        best_train_loss = avg_train_loss\n",
    "        \n",
    "    early_stopper(avg_val_loss)\n",
    "    if early_stopper.early_stop:\n",
    "        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "X_test = torch.load(test_feat_path)\n",
    "y_test = torch.load(test_label_path)\n",
    "\n",
    "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "test_loss = 0\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in test_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        preds = model(x_batch)\n",
    "        loss = criterion(preds, y_batch)\n",
    "\n",
    "        test_loss += loss.item() * x_batch.size(0)\n",
    "        _, predicted = torch.max(preds, 1)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "\n",
    "avg_test_loss = test_loss / total\n",
    "test_accuracy = correct / total\n",
    "\n",
    "print(f\"\\nTest Loss: {avg_test_loss:.4f} | Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fb0d19",
   "metadata": {},
   "source": [
    "Classification Report "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539ef270",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "y_pred = []\n",
    "if 'model' in locals() and 'test_loader' in locals():\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    for x_batch, _ in test_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        with torch.no_grad():\n",
    "            preds = model(x_batch)\n",
    "        _, predicted = torch.max(preds, 1)\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "    report = metrics.classification_report(y_test.numpy(), y_pred, digits=4)\n",
    "    print(report)\n",
    "    with open(\"classification_report.txt\", \"w\") as f:\n",
    "        f.write(report)\n",
    "    print(\"Full classification report saved to classification_report.txt\")\n",
    "else:\n",
    "    print(\"Model or test_loader not found. Please run previous cells to train and load model/test data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38d451d",
   "metadata": {},
   "source": [
    "To identify systematic misclassifications between semantically similar classes, we perform a confusion pair analysis on the normalized confusion matrix $ \\mathbf{C}_{\\text{norm}} $, where each entry $ C_{ij} $ represents the proportion of true class $ i $ samples predicted as class $ j $.\n",
    "The diagonal elements (correct predictions) are set to zero to focus exclusively on misclassification patterns. We then compute the mean $ \\mu $ and standard deviation $ \\sigma $ of all non-zero off-diagonal entries.\n",
    "A significance threshold is defined as:\n",
    "$$T = \\mu + k \\cdot \\sigma$$\n",
    "where $ k $ is a sensitivity parameter (typically $ k = 2 $, corresponding to the empirical rule â€” approximately 95% of normally distributed data lies within 2 standard deviations of the mean; see e.g., Devore, 2015).\n",
    "Class pairs $ (i, j) $ with $ C_{ij} \\geq T $ are flagged as statistically significant confusion pairs. Directional duplicates (i.e., $ i \\to j $ and $ j \\to i $) are merged into undirected symmetric pairs to avoid redundancy.\n",
    "For each significant pair, we extract precision, recall, F1-score, and support from the classification report to provide deeper insight into model behavior.\n",
    "This method effectively surfaces semantically or visually confusable classes, enabling targeted interventions such as dataset augmentation, architecture refinement, or interpretability analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747816fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "import pandas as pd\n",
    "\n",
    "STDEV_SENSITIVITY = 2.0\n",
    "TEST_SET_DIR = 'test_set'\n",
    "\n",
    "temp_test_dataset = datasets.ImageFolder(root=TEST_SET_DIR)\n",
    "idx_to_class = {v: k for k, v in temp_test_dataset.class_to_idx.items()}\n",
    "\n",
    "y_test_np = y_test.numpy()\n",
    "y_pred_np = np.array(y_pred)\n",
    "\n",
    "cm = metrics.confusion_matrix(y_test_np, y_pred_np)\n",
    "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "np.fill_diagonal(cm_norm, 0)\n",
    "\n",
    "non_zero = cm_norm.flatten()[cm_norm.flatten() > 0]\n",
    "threshold = np.mean(non_zero) + STDEV_SENSITIVITY * np.std(non_zero)\n",
    "\n",
    "confusions = []\n",
    "seen_pairs = set()\n",
    "for i in range(cm_norm.shape[0]):\n",
    "    for j in range(cm_norm.shape[0]):\n",
    "        if i != j and cm_norm[i, j] >= threshold:\n",
    "            pair = tuple(sorted([i, j]))\n",
    "            if pair not in seen_pairs:\n",
    "                seen_pairs.add(pair)\n",
    "                true_cls = idx_to_class[i]\n",
    "                pred_cls = idx_to_class[j]\n",
    "                rate = cm_norm[i, j]\n",
    "                confusions.append((true_cls, pred_cls, rate))\n",
    "\n",
    "confusions.sort(key=lambda x: x[2], reverse=True)\n",
    "top5 = confusions[:5]\n",
    "\n",
    "report = metrics.classification_report(y_test_np, y_pred_np, target_names=[idx_to_class[i] for i in range(len(idx_to_class))], output_dict=True)\n",
    "\n",
    "rows = []\n",
    "for true_cls, pred_cls, rate in top5:\n",
    "    precision = report[pred_cls]['precision']\n",
    "    recall_true = report[true_cls]['recall']\n",
    "    f1_true = report[true_cls]['f1-score']\n",
    "    f1_pred = report[pred_cls]['f1-score']\n",
    "    support = report[true_cls]['support']\n",
    "    rows.append({\n",
    "        'True': true_cls,\n",
    "        'Pred': pred_cls,\n",
    "        'MisRate': f'{rate:.4f}',\n",
    "        'Precision(Pred)': f'{precision:.3f}',\n",
    "        'Recall(True)': f'{recall_true:.3f}',\n",
    "        'F1(True)': f'{f1_true:.3f}',\n",
    "        'F1(Pred)': f'{f1_pred:.3f}',\n",
    "        'Support(True)': support\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print(f\"Found {len(top5)} unique significant confusion pairs (threshold = {threshold:.4f})\")\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb3a68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import random\n",
    "import os\n",
    "\n",
    "def show_confusion_pair_images(top5, test_set_dir, y_test_np, y_pred_np, idx_to_class):\n",
    "    temp_test_dataset = datasets.ImageFolder(root=test_set_dir)\n",
    "    samples = temp_test_dataset.samples  \n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    for i, (true_cls, pred_cls, _) in enumerate(top5):\n",
    "        true_idx = list(idx_to_class.keys())[list(idx_to_class.values()).index(true_cls)]\n",
    "        pred_idx = list(idx_to_class.keys())[list(idx_to_class.values()).index(pred_cls)]\n",
    "        mask = (y_test_np == true_idx) & (y_pred_np == pred_idx)\n",
    "        indices = np.where(mask)[0]\n",
    "        if len(indices) == 0:\n",
    "            print(f\"No sample found for pair: {true_cls} -> {pred_cls}\")\n",
    "            continue\n",
    "        idx = random.choice(indices)\n",
    "        img_path, _ = samples[idx]\n",
    "        img = Image.open(img_path)\n",
    "        plt.subplot(3, 2, i+1)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.title(f'True: {true_cls}\\nPred: {pred_cls}')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show_confusion_pair_images(top5, TEST_SET_DIR, y_test_np, y_pred_np, idx_to_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adefa7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "X_train = torch.load(\"new_dataset_features/train_X_dino.pt\").numpy()\n",
    "y_train = torch.load(\"new_dataset_features/train_y.pt\").numpy()\n",
    "\n",
    "class_name_to_idx = {v: k for k, v in idx_to_class.items()}\n",
    "\n",
    "for i, (true_cls, pred_cls, _) in enumerate(top5):\n",
    "    true_idx = class_name_to_idx[true_cls] if isinstance(true_cls, str) else true_cls\n",
    "    pred_idx = class_name_to_idx[pred_cls] if isinstance(pred_cls, str) else pred_cls\n",
    "    \n",
    "    mask = np.isin(y_train, [true_idx, pred_idx])\n",
    "    X_pair = X_train[mask]\n",
    "    y_pair = y_train[mask]\n",
    "    \n",
    "    reducer = umap.UMAP(\n",
    "        n_neighbors=30,\n",
    "        min_dist=0.1,\n",
    "        metric='cosine',\n",
    "        random_state=42\n",
    ")\n",
    "    X_embedded = reducer.fit_transform(X_pair)\n",
    "    \n",
    "    plt.figure(figsize=(7, 5))\n",
    "    colors = [plt.cm.tab10(0), plt.cm.tab10(1)]\n",
    "    for idx, color, label in zip([true_idx, pred_idx], colors, [true_cls, pred_cls]):\n",
    "        plt.scatter(X_embedded[y_pair == idx, 0], X_embedded[y_pair == idx, 1], \n",
    "                    c=[color], label=label, s=14, alpha=0.7)\n",
    "    plt.title(f\"UMAP: {true_cls} vs {pred_cls}\")\n",
    "    plt.xlabel(\"UMAP-1\")\n",
    "    plt.ylabel(\"UMAP-2\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
